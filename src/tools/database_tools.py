# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

"""
æ•°æ®åº“æ“ä½œå·¥å…·

ä¸ºæ™ºèƒ½ä½“æä¾›æ•°æ®åº“æŸ¥è¯¢å’Œç®¡ç†åŠŸèƒ½
"""

import json
import logging
import asyncio
import pandas as pd
from typing import Dict, Any, Optional, List, Annotated
from datetime import datetime

from langchain_core.tools import tool
from src.database import get_db_session
from src.tools.decorators import log_io
from src.services.database_datasource import DatabaseDatasourceService
from src.services.websocket.progress_manager import progress_ws_manager

logger = logging.getLogger(__name__)


class ToolResult:
    """ç»Ÿä¸€çš„å·¥å…·è¿”å›å€¼ç»“æ„"""
    
    def __init__(
        self, 
        success: bool, 
        message: str, 
        data: Any = None, 
        error: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ):
        self.success = success
        self.message = message
        self.data = data
        self.error = error
        self.metadata = metadata or {}
        self.timestamp = datetime.now().isoformat()
    
    def to_json(self) -> str:
        """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
        return json.dumps({
            "success": self.success,
            "message": self.message,
            "data": self.data,
            "error": self.error,
            "metadata": self.metadata,
            "timestamp": self.timestamp
        }, ensure_ascii=False, indent=2)


@tool
@log_io
def database_query(
    database_name: Annotated[str, "æ•°æ®åº“åç§°æˆ–ID"],
    query_type: Annotated[str, "æŸ¥è¯¢ç±»å‹: info|tables|schema"] = "info"
) -> str:
    """
    æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢

    æ”¯æŒå¤šç§æŸ¥è¯¢ç±»å‹ï¼šè·å–æ•°æ®åº“ä¿¡æ¯ã€è¡¨åˆ—è¡¨ã€è¡¨ç»“æ„ã€‚
    æ³¨æ„ï¼šä¸æ”¯æŒSQLæ‰§è¡Œï¼Œè¯·ä½¿ç”¨text2sql_queryå·¥å…·æ‰§è¡ŒSQLæŸ¥è¯¢ã€‚
    """
    try:
        logger.info(f"ğŸ—„ï¸ æ•°æ®åº“æŸ¥è¯¢: {database_name} - {query_type}")

        # ä½¿ç”¨çœŸå®çš„æ•°æ®åº“æ•°æ®æºæœåŠ¡
        datasource_service = DatabaseDatasourceService()

        # è·å–æ•°æ®åº“é…ç½®
        if database_name.isdigit():
            datasource = asyncio.run(datasource_service.get_datasource(int(database_name)))
        else:
            # æŒ‰åç§°æŸ¥æ‰¾éœ€è¦å…ˆåˆ—å‡ºæ‰€æœ‰æ•°æ®æº
            datasources = asyncio.run(datasource_service.list_datasources(search=database_name))
            datasource = datasources[0] if datasources else None

        if not datasource:
            return ToolResult(
                success=False,
                message=f"æ•°æ®åº“ '{database_name}' æœªæ‰¾åˆ°",
                error="DATABASE_NOT_FOUND"
            ).to_json()

        # æ ¹æ®æŸ¥è¯¢ç±»å‹æ‰§è¡Œä¸åŒæ“ä½œ
        if query_type == "info":
            return ToolResult(
                success=True,
                message=f"æ•°æ®åº“ '{datasource.name}' ä¿¡æ¯è·å–æˆåŠŸ",
                data={
                    "id": datasource.id,
                    "name": datasource.name,
                    "description": datasource.description,
                    "type": datasource.database_type.value,
                    "host": datasource.host,
                    "port": datasource.port,
                    "database": datasource.database_name
                }
            ).to_json()

        elif query_type == "tables":
            # è·å–æ•°æ®åº“ç»“æ„ä¿¡æ¯
            schema_response = asyncio.run(datasource_service.get_database_schema(datasource.id))
            if not schema_response:
                return ToolResult(
                    success=False,
                    message=f"æ— æ³•è·å–æ•°æ®åº“ '{datasource.name}' çš„è¡¨åˆ—è¡¨",
                    error="SCHEMA_EXTRACTION_FAILED"
                ).to_json()

            tables = []
            for table in schema_response.tables:
                table_info = {
                    "name": table.get("table_name", ""),
                    "schema": table.get("schema_name", ""),
                    "column_count": table.get("column_count", 0)
                }
                tables.append(table_info)

            return ToolResult(
                success=True,
                message=f"æ•°æ®åº“ '{datasource.name}' è¡¨åˆ—è¡¨è·å–æˆåŠŸ",
                data={"tables": tables, "count": len(tables)},
                metadata={"database_id": datasource.id, "total_tables": schema_response.total_tables}
            ).to_json()

        elif query_type == "schema":
            # è·å–æ•°æ®åº“ç»“æ„ä¿¡æ¯
            schema_response = asyncio.run(datasource_service.get_database_schema(datasource.id))
            if not schema_response:
                return ToolResult(
                    success=False,
                    message=f"æ— æ³•è·å–æ•°æ®åº“ '{datasource.name}' çš„ç»“æ„ä¿¡æ¯",
                    error="SCHEMA_EXTRACTION_FAILED"
                ).to_json()

            schema_info = {}
            for table in schema_response.tables:
                table_name = table.get("table_name", "")
                schema_info[table_name] = {
                    "columns": table.get("columns", []),
                    "column_count": table.get("column_count", 0),
                    "schema_name": table.get("schema_name", "")
                }

            return ToolResult(
                success=True,
                message=f"æ•°æ®åº“ '{datasource.name}' ç»“æ„ä¿¡æ¯è·å–æˆåŠŸ",
                data={"schema": schema_info, "table_count": len(schema_info)},
                metadata={"database_id": datasource.id, "total_tables": schema_response.total_tables}
            ).to_json()

        else:
            return ToolResult(
                success=False,
                message=f"ä¸æ”¯æŒçš„æŸ¥è¯¢ç±»å‹: {query_type}ã€‚æ”¯æŒçš„ç±»å‹: info, tables, schema",
                error="INVALID_QUERY_TYPE"
            ).to_json()
    
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¼‚å¸¸: {e}")
        return ToolResult(
            success=False,
            message="æ•°æ®åº“æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸",
            error=str(e),
            metadata={"database_name": database_name, "query_type": query_type}
        ).to_json()


@tool
@log_io
def search_databases(
    search_term: Annotated[str, "æœç´¢å…³é”®è¯ï¼Œå¯ä»¥æ˜¯æ•°æ®åº“åç§°ã€æè¿°æˆ–ç±»å‹"] = "",
    database_type: Annotated[str, "æ•°æ®åº“ç±»å‹è¿‡æ»¤ (mysql, postgresql)"] = "",
    enabled_only: Annotated[bool, "åªæ˜¾ç¤ºå¯ç”¨çš„æ•°æ®åº“"] = True
) -> str:
    """
    æœç´¢å’Œå‘ç°æ•°æ®åº“

    æ ¹æ®åç§°ã€æè¿°ã€ç±»å‹ç­‰æ¡ä»¶æœç´¢æ•°æ®åº“ã€‚æ”¯æŒæ¨¡ç³ŠåŒ¹é…ã€‚
    """
    try:
        logger.info(f"ğŸ” æœç´¢æ•°æ®åº“: search_term='{search_term}', type='{database_type}', enabled_only={enabled_only}")

        # ä½¿ç”¨æ•°æ®åº“æ•°æ®æºæœåŠ¡è¿›è¡Œæœç´¢
        datasource_service = DatabaseDatasourceService()

        # è½¬æ¢æ•°æ®åº“ç±»å‹
        db_type_filter = None
        if database_type:
            from src.models.database_datasource import DatabaseType
            try:
                db_type_filter = DatabaseType(database_type.upper())
            except ValueError:
                logger.warning(f"æ— æ•ˆçš„æ•°æ®åº“ç±»å‹: {database_type}")

        # æ‰§è¡Œæœç´¢
        datasources = asyncio.run(datasource_service.list_datasources(
            database_type=db_type_filter,
            search=search_term if search_term else None,
            limit=50,
            offset=0
        ))

        # è¿‡æ»¤å¯ç”¨çŠ¶æ€
        if enabled_only:
            datasources = [ds for ds in datasources if ds.deleted_at is None]

        # æ„å»ºç»“æœ
        databases = []
        for ds in datasources:
            db_info = {
                "id": ds.id,
                "name": ds.name,
                "description": ds.description,
                "type": ds.database_type.value,
                "host": ds.host,
                "port": ds.port,
                "database": ds.database_name,
                "enabled": ds.deleted_at is None,
                "created_at": ds.created_at.isoformat() if ds.created_at else None
            }
            databases.append(db_info)

        # è®¡ç®—åŒ¹é…åº¦è¯„åˆ†ï¼ˆç”¨äºæ’åºï¼‰
        if search_term:
            for db in databases:
                score = 0
                search_lower = search_term.lower()

                # åç§°å®Œå…¨åŒ¹é…å¾—åˆ†æœ€é«˜
                if db["name"].lower() == search_lower:
                    score += 100
                elif search_lower in db["name"].lower():
                    score += 50

                # æè¿°åŒ¹é…
                if db["description"] and search_lower in db["description"].lower():
                    score += 30

                # ç±»å‹åŒ¹é…
                if search_lower in db["type"].lower():
                    score += 20

                db["match_score"] = score

            # æŒ‰åŒ¹é…åº¦æ’åº
            databases.sort(key=lambda x: x.get("match_score", 0), reverse=True)

        return ToolResult(
            success=True,
            message=f"æ‰¾åˆ° {len(databases)} ä¸ªåŒ¹é…çš„æ•°æ®åº“",
            data={
                "databases": databases,
                "total_count": len(databases),
                "search_criteria": {
                    "search_term": search_term,
                    "database_type": database_type,
                    "enabled_only": enabled_only
                }
            },
            metadata={"search_performed": bool(search_term)}
        ).to_json()

    except Exception as e:
        logger.error(f"âŒ æœç´¢æ•°æ®åº“å¼‚å¸¸: {e}")
        return ToolResult(
            success=False,
            message="æœç´¢æ•°æ®åº“è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸",
            error=str(e),
            metadata={"search_term": search_term, "database_type": database_type}
        ).to_json()


@tool
@log_io
def list_databases(
    enabled_only: Annotated[bool, "åªæ˜¾ç¤ºå¯ç”¨çš„æ•°æ®åº“"] = True
) -> str:
    """
    åˆ—å‡ºå¯ç”¨çš„æ•°æ®åº“

    è·å–ç³»ç»Ÿä¸­é…ç½®çš„æ‰€æœ‰æ•°æ®åº“è¿æ¥ä¿¡æ¯ã€‚
    """
    try:
        logger.info(f"ğŸ“‹ è·å–æ•°æ®åº“åˆ—è¡¨: enabled_only={enabled_only}")
        
        session = next(get_db_session())
        
        try:
            from sqlalchemy import text
            
            where_clause = "WHERE deleted_at IS NULL" if enabled_only else ""
            
            query = text(f"""
                SELECT id, name, description, database_type, host, port,
                       database_name, (deleted_at IS NULL) as enabled, created_at
                FROM database_management.database_datasources
                {where_clause}
                ORDER BY name
            """)
            
            result = session.execute(query)
            databases = []
            
            for row in result.fetchall():
                db_info = {
                    "id": row.id,
                    "name": row.name,
                    "description": row.description,
                    "type": row.database_type,
                    "host": row.host,
                    "port": row.port,
                    "database": row.database_name,
                    "enabled": row.enabled,
                    "created_at": row.created_at.isoformat() if row.created_at else None
                }
                databases.append(db_info)
            
            return ToolResult(
                success=True,
                message=f"æ‰¾åˆ° {len(databases)} ä¸ªæ•°æ®åº“",
                data={"databases": databases, "total_count": len(databases)},
                metadata={"enabled_only": enabled_only}
            ).to_json()
        
        finally:
            session.close()
    
    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®åº“åˆ—è¡¨å¼‚å¸¸: {e}")
        return ToolResult(
            success=False,
            message="è·å–æ•°æ®åº“åˆ—è¡¨è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸",
            error=str(e)
        ).to_json()


@tool
@log_io
def test_database_connection(
    database_name: Annotated[str, "æ•°æ®åº“åç§°æˆ–ID"]
) -> str:
    """
    æµ‹è¯•æ•°æ®åº“è¿æ¥
    
    éªŒè¯æŒ‡å®šæ•°æ®åº“çš„è¿æ¥çŠ¶æ€å’Œå¯ç”¨æ€§ã€‚
    """
    try:
        logger.info(f"ğŸ”Œ æµ‹è¯•æ•°æ®åº“è¿æ¥: {database_name}")
        
        session = next(get_db_session())
        
        try:
            from sqlalchemy import text
            
            # æŸ¥æ‰¾æ•°æ®åº“é…ç½®
            if database_name.isdigit():
                query = text("""
                    SELECT id, name, host, port, database_name, database_type
                    FROM database_management.database_datasources 
                    WHERE id = :db_id
                """)
                result = session.execute(query, {"db_id": int(database_name)})
            else:
                query = text("""
                    SELECT id, name, host, port, database_name, database_type
                    FROM database_management.database_datasources 
                    WHERE name = :db_name
                """)
                result = session.execute(query, {"db_name": database_name})
            
            db_config = result.fetchone()
            if not db_config:
                return ToolResult(
                    success=False,
                    message=f"æ•°æ®åº“ '{database_name}' æœªæ‰¾åˆ°",
                    error="DATABASE_NOT_FOUND"
                ).to_json()
            
            # ç®€å•çš„è¿æ¥æµ‹è¯•ï¼ˆä½¿ç”¨å½“å‰è¿æ¥æ‰§è¡ŒåŸºæœ¬æŸ¥è¯¢ï¼‰
            test_query = text("SELECT 1 as test_connection")
            test_result = session.execute(test_query)
            test_row = test_result.fetchone()
            
            if test_row and test_row.test_connection == 1:
                return ToolResult(
                    success=True,
                    message=f"æ•°æ®åº“ '{db_config.name}' è¿æ¥æµ‹è¯•æˆåŠŸ",
                    data={
                        "database_id": db_config.id,
                        "database_name": db_config.name,
                        "host": db_config.host,
                        "port": db_config.port,
                        "database": db_config.database_name,
                        "type": db_config.database_type,
                        "connection_status": "success"
                    }
                ).to_json()
            else:
                return ToolResult(
                    success=False,
                    message=f"æ•°æ®åº“ '{db_config.name}' è¿æ¥æµ‹è¯•å¤±è´¥",
                    error="CONNECTION_TEST_FAILED"
                ).to_json()
        
        finally:
            session.close()
    
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
        return ToolResult(
            success=False,
            message="æ•°æ®åº“è¿æ¥æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸",
            error=str(e),
            metadata={"database_name": database_name}
        ).to_json()


@tool
@log_io
def find_database_by_name(
    name_pattern: Annotated[str, "æ•°æ®åº“åç§°æ¨¡å¼ï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…"]
) -> str:
    """
    æ ¹æ®åç§°æ¨¡å¼æŸ¥æ‰¾æ•°æ®åº“

    æ™ºèƒ½åŒ¹é…æ•°æ®åº“åç§°ï¼Œæ”¯æŒéƒ¨åˆ†åŒ¹é…å’Œæ¨¡ç³Šæœç´¢ã€‚
    ç‰¹åˆ«é€‚ç”¨äºæ ¹æ®ä¸šåŠ¡åç§°æŸ¥æ‰¾å¯¹åº”çš„æ•°æ®åº“ã€‚
    """
    try:
        logger.info(f"ğŸ¯ æ ¹æ®åç§°æŸ¥æ‰¾æ•°æ®åº“: {name_pattern}")

        # ä½¿ç”¨æ•°æ®åº“æ•°æ®æºæœåŠ¡è¿›è¡Œæœç´¢
        datasource_service = DatabaseDatasourceService()

        # æ‰§è¡Œæœç´¢
        datasources = asyncio.run(datasource_service.list_datasources(
            search=name_pattern,
            limit=20,
            offset=0
        ))

        # åªè¿”å›å¯ç”¨çš„æ•°æ®åº“
        datasources = [ds for ds in datasources if ds.deleted_at is None]

        if not datasources:
            return ToolResult(
                success=False,
                message=f"æœªæ‰¾åˆ°åŒ¹é… '{name_pattern}' çš„æ•°æ®åº“",
                error="NO_MATCHING_DATABASE"
            ).to_json()

        # æ„å»ºç»“æœå¹¶è®¡ç®—åŒ¹é…åº¦
        matches = []
        pattern_lower = name_pattern.lower()

        for ds in datasources:
            match_info = {
                "id": ds.id,
                "name": ds.name,
                "description": ds.description,
                "type": ds.database_type.value,
                "host": ds.host,
                "port": ds.port,
                "database": ds.database_name,
                "match_type": [],
                "match_score": 0
            }

            # è®¡ç®—åŒ¹é…ç±»å‹å’Œå¾—åˆ†
            if ds.name.lower() == pattern_lower:
                match_info["match_type"].append("exact_name")
                match_info["match_score"] += 100
            elif pattern_lower in ds.name.lower():
                match_info["match_type"].append("partial_name")
                match_info["match_score"] += 80

            if ds.description and pattern_lower in ds.description.lower():
                match_info["match_type"].append("description")
                match_info["match_score"] += 50

            if pattern_lower in ds.database_name.lower():
                match_info["match_type"].append("database_name")
                match_info["match_score"] += 60

            matches.append(match_info)

        # æŒ‰åŒ¹é…åº¦æ’åº
        matches.sort(key=lambda x: x["match_score"], reverse=True)

        # å¦‚æœæœ‰å®Œå…¨åŒ¹é…ï¼Œä¼˜å…ˆè¿”å›
        exact_matches = [m for m in matches if "exact_name" in m["match_type"]]
        if exact_matches:
            best_match = exact_matches[0]
            return ToolResult(
                success=True,
                message=f"æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„æ•°æ®åº“: {best_match['name']}",
                data={
                    "best_match": best_match,
                    "all_matches": matches,
                    "match_count": len(matches)
                },
                metadata={"search_pattern": name_pattern, "match_type": "exact"}
            ).to_json()

        # è¿”å›æœ€ä½³åŒ¹é…
        best_match = matches[0]
        return ToolResult(
            success=True,
            message=f"æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…çš„æ•°æ®åº“ï¼Œæœ€ä½³åŒ¹é…: {best_match['name']}",
            data={
                "best_match": best_match,
                "all_matches": matches,
                "match_count": len(matches)
            },
            metadata={"search_pattern": name_pattern, "match_type": "fuzzy"}
        ).to_json()

    except Exception as e:
        logger.error(f"âŒ æ ¹æ®åç§°æŸ¥æ‰¾æ•°æ®åº“å¼‚å¸¸: {e}")
        return ToolResult(
            success=False,
            message="æ ¹æ®åç§°æŸ¥æ‰¾æ•°æ®åº“è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸",
            error=str(e),
            metadata={"name_pattern": name_pattern}
        ).to_json()


@tool
@log_io
def get_database_info(
    identifier: Annotated[str, "æ•°æ®åº“æ ‡è¯†ç¬¦ï¼Œå¯ä»¥æ˜¯IDã€åç§°æˆ–æè¿°å…³é”®è¯"]
) -> str:
    """
    è·å–æ•°æ®åº“è¯¦ç»†ä¿¡æ¯

    æ ¹æ®IDã€åç§°æˆ–æè¿°å…³é”®è¯è·å–æ•°æ®åº“çš„å®Œæ•´ä¿¡æ¯ã€‚
    è¿™æ˜¯ä¸€ä¸ªæ™ºèƒ½æŸ¥æ‰¾å·¥å…·ï¼Œä¼šå°è¯•å¤šç§åŒ¹é…æ–¹å¼ã€‚
    """
    try:
        logger.info(f"ğŸ“‹ è·å–æ•°æ®åº“ä¿¡æ¯: {identifier}")

        datasource_service = DatabaseDatasourceService()
        datasource = None

        # å°è¯•æŒ‰IDæŸ¥æ‰¾
        if identifier.isdigit():
            try:
                datasource = asyncio.run(datasource_service.get_datasource(int(identifier)))
                if datasource:
                    logger.info(f"é€šè¿‡IDæ‰¾åˆ°æ•°æ®åº“: {datasource.name}")
            except Exception as e:
                logger.debug(f"æŒ‰IDæŸ¥æ‰¾å¤±è´¥: {e}")

        # å¦‚æœæŒ‰IDæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŒ‰åç§°æœç´¢
        if not datasource:
            datasources = asyncio.run(datasource_service.list_datasources(
                search=identifier,
                limit=10,
                offset=0
            ))

            # è¿‡æ»¤å¯ç”¨çš„æ•°æ®åº“
            enabled_datasources = [ds for ds in datasources if ds.deleted_at is None]

            if enabled_datasources:
                # ä¼˜å…ˆé€‰æ‹©åç§°å®Œå…¨åŒ¹é…çš„
                exact_match = next((ds for ds in enabled_datasources
                                  if ds.name.lower() == identifier.lower()), None)
                datasource = exact_match or enabled_datasources[0]
                logger.info(f"é€šè¿‡æœç´¢æ‰¾åˆ°æ•°æ®åº“: {datasource.name}")

        if not datasource:
            return ToolResult(
                success=False,
                message=f"æœªæ‰¾åˆ°æ•°æ®åº“ '{identifier}'",
                error="DATABASE_NOT_FOUND"
            ).to_json()

        # è·å–æ•°æ®åº“ç»“æ„ä¿¡æ¯
        try:
            schema_response = asyncio.run(datasource_service.get_database_schema(datasource.id))
            table_count = len(schema_response.tables) if schema_response else 0
            table_names = [table.get("table_name", "") for table in schema_response.tables] if schema_response else []
        except Exception as e:
            logger.warning(f"è·å–æ•°æ®åº“ç»“æ„å¤±è´¥: {e}")
            table_count = 0
            table_names = []

        # æ„å»ºè¯¦ç»†ä¿¡æ¯
        db_info = {
            "basic_info": {
                "id": datasource.id,
                "name": datasource.name,
                "description": datasource.description,
                "type": datasource.database_type.value,
                "host": datasource.host,
                "port": datasource.port,
                "database": datasource.database_name,
                "enabled": datasource.deleted_at is None,
                "created_at": datasource.created_at.isoformat() if datasource.created_at else None
            },
            "schema_info": {
                "table_count": table_count,
                "table_names": table_names[:10],  # åªæ˜¾ç¤ºå‰10ä¸ªè¡¨å
                "has_more_tables": table_count > 10
            },
            "connection_info": {
                "readonly_mode": datasource.readonly_mode,
                "allowed_operations": datasource.allowed_operations
            }
        }

        return ToolResult(
            success=True,
            message=f"æ•°æ®åº“ '{datasource.name}' ä¿¡æ¯è·å–æˆåŠŸ",
            data=db_info,
            metadata={
                "identifier": identifier,
                "found_by": "id" if identifier.isdigit() else "search"
            }
        ).to_json()

    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®åº“ä¿¡æ¯å¼‚å¸¸: {e}")
        return ToolResult(
            success=False,
            message="è·å–æ•°æ®åº“ä¿¡æ¯è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸",
            error=str(e),
            metadata={"identifier": identifier}
        ).to_json()


def _auto_generate_chart_config(data: List[Dict], title: str = "æ•°æ®å›¾è¡¨") -> Optional[Dict]:
    """
    æ ¹æ®æ•°æ®è‡ªåŠ¨ç”Ÿæˆå›¾è¡¨é…ç½®
    """
    if not data or len(data) < 2:
        return None

    try:
        df = pd.DataFrame(data)

        # æ£€æµ‹æ•°å€¼åˆ—å’Œåˆ†ç±»åˆ—
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

        if not numeric_cols:
            return None

        # æ£€æµ‹æ—¶é—´åˆ—
        time_cols = [col for col in df.columns if any(keyword in col.lower()
                    for keyword in ['time', 'date', 'æ—¶é—´', 'æ—¥æœŸ', 'created', 'updated'])]

        # é€‰æ‹©å›¾è¡¨ç±»å‹å’Œé…ç½®
        if time_cols and len(numeric_cols) >= 1:
            # æ—¶é—´åºåˆ— -> æŠ˜çº¿å›¾
            return {
                "type": "LineChart",
                "title": title,
                "width": 800,
                "height": 400,
                "data": data,
                "xAxis": {"dataKey": time_cols[0], "type": "category"},
                "yAxis": {"type": "number"},
                "lines": [{"dataKey": numeric_cols[0], "stroke": "#8884d8", "type": "monotone"}],
                "tooltip": {"active": True},
                "legend": True
            }
        elif categorical_cols and numeric_cols:
            # åˆ†ç±»æ•°æ® -> æŸ±çŠ¶å›¾
            return {
                "type": "BarChart",
                "title": title,
                "width": 800,
                "height": 400,
                "data": data,
                "xAxis": {"dataKey": categorical_cols[0], "type": "category"},
                "yAxis": {"type": "number"},
                "bars": [{"dataKey": numeric_cols[0], "fill": "#8884d8"}],
                "tooltip": {"active": True},
                "legend": True
            }
        elif len(numeric_cols) >= 1 and len(df) <= 50:
            # çº¯æ•°å€¼æ•°æ® -> å¸¦ç´¢å¼•çš„æŸ±çŠ¶å›¾
            data_with_index = [{"åºå·": f"ç¬¬{i+1}é¡¹", **row} for i, row in enumerate(data)]
            return {
                "type": "BarChart",
                "title": title,
                "width": 800,
                "height": 400,
                "data": data_with_index,
                "xAxis": {"dataKey": "åºå·", "type": "category"},
                "yAxis": {"type": "number"},
                "bars": [{"dataKey": numeric_cols[0], "fill": "#8884d8"}],
                "tooltip": {"active": True},
                "legend": True
            }

        return None

    except Exception as e:
        logger.warning(f"è‡ªåŠ¨å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
        return None
